{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636bf905-4d45-4fdf-8ee7-41493ea5751a",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bfec1-9efb-47f1-beb8-586d1d0ccc5a",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to **characterize bus repairs** from free text descriptions, entered by users. This will be accomplished through the use of **Natural Language Processing (NLP)**.\n",
    "\n",
    "This will allow you to discover, step by step, how you can create the code doing the repair text processing.  In the last part of the workshop, this code will be **packaged to create a service** that you can query from an application.\n",
    "\n",
    "We will be training the model.  Once our model is trained, we can test the model by entering a bus repair issue (e.g. the brakes feel soft when I press on them) and check if the model has correctly characterized the claim.  Repairs will be categorized as:  Brakes, Starter or Other.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1c712-4f58-4d9c-93ff-39222636d5bf",
   "metadata": {},
   "source": [
    "# Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadbffe-164d-40e7-a66e-36c5e0cd4718",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "First, we'll need to **install some libraries** that are not part of our container image. Normally, **Red Hat OpenShift Data Science** is already taking care of this for you, based on what it detects in the code. **Red Hat OpenShift Data Science** will reinstall all those libraries for you every time you launch the notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79991b4-2112-4fd0-b2dc-5bec5a25ed59",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Of course, we'll need to import various packages. They are either built in the notebook image you are running, or have been installed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87a88b-e1bc-4bf9-b756-818c8bb69e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f6f8e-6f58-4d47-b818-c6983e9e73fc",
   "metadata": {},
   "source": [
    "## Create Training and Testing data sets\n",
    "\n",
    "Now that we have loaded the tools we need, the first step in our journey is to be able to take our raw data and divide it into testing and training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d83e0-8968-4ad8-9fea-56d881637b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "#Determine what the training and testing percentages will be.\n",
    "#============================================================================\n",
    "\n",
    "training_portion = .80  # Use 80% of data for training, 20% for testing\n",
    "max_words = 1000        #Max words in text input\n",
    "\n",
    "data = pd.read_csv('testdata1.csv')\n",
    "\n",
    "#print out first 5 rows of generated claims so you can see what data looks like\n",
    "print(data.head())\n",
    "\n",
    "train_size = int(len(data) * training_portion)\n",
    "\n",
    "#============================================================================\n",
    "# FUNCTION:  train_test_split - splits the data into training and test sets.  \n",
    "# Inputs are the raw data and determined train_size.\n",
    "#============================================================================\n",
    "def train_test_split(data, train_size):\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    return train, test\n",
    "\n",
    "train_cat, test_cat = train_test_split(data.iloc[:,1], train_size)  # label data is second column\n",
    "train_text, test_text = train_test_split(data.iloc[:,0], train_size)  # text data is first column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0af16-5205-4479-84f8-8dce26e328b0",
   "metadata": {},
   "source": [
    "## Tokenize the Data sets\n",
    "\n",
    "After we have training and testing sets, we need to **tokenize the data**.  This means that we convert text documents into contextual vectors which contain numeric representations (index of where those words occur in a word dictionary) of the words in the documents.\n",
    "\n",
    "To see how Tokenization works, you can take a look at **02-TokenDemo.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d01f6a-4e86-4ce6-bc37-b31b7747a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
    "\n",
    "#============================================================================\n",
    "#x_train and x_test are the vectorization of the text data (which is a claim)\n",
    "#============================================================================\n",
    "x_train = tokenize.texts_to_matrix(train_text)\n",
    "\n",
    "#following print statement shows some rows in the newly created matrix\n",
    "print(x_train)\n",
    "x_test = tokenize.texts_to_matrix(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1751d740-cc58-4d1a-8251-dcf5cc3ebc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 1 1 0 0 0 1 1 0 1 0 2 1 2 1 1 0 2 1 2 0 1 1 1 1 0 2 1 0 2 2 1 0 2 1\n",
      " 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 2 1 2 1 0 0 1 1 1 1 1 1 2\n",
      " 0 1 0 1 1 2 1 0 0 0 0 1 2 1 1 1 1 2 2 1 2 0 1 0 2 1 2 1 1 2 1 1 1 2 0 1 1\n",
      " 1 0 0 1 0 1 1 2 0 0 1 2 0 1 1 0 1 1 1 2 0 0 0 2 2 0 0 2 2 1 1 0 0 2 1 0 0\n",
      " 0 1 2 2 0 2 0 1 2 2 2 2 1 1 0 1 1 2 0 1 2 0 1 2 1 1 0 0 1 1 1 2 1 1 0 1 0\n",
      " 2 2 0 2 1 2 1 2 0 1 1 0 0 1 1 2 1 1 0 0 1 2 2 1 1 1 2 0 2 2 1 1 2 1 0 1 2\n",
      " 1 0 2 1 1 0 1 1 1 2 1 0 1 1 1 2 2 2 1 1 1 1 1 2 0 0 2 0 1 2 2 2 1 1 2 2 1\n",
      " 1 1 2 2 1 2 2 1 1 1 1 1 2 2 1 1 1 0 0 1 1 0 1 0 1 1 0 2 1 2 2 1 1 2 2 1 1\n",
      " 0 1 1 2 1 1 2 2 0 0 0 2 0 1 1 1 1 2 1 1 1 0 1 0 0 2 1 2 2 2 0 2 0 1 0 0 1\n",
      " 1 1 0 2 0 0 1 2 1 2 1 1 1 2 1 1 0 1 1 0 0 2 2 1 1 1 2 1 0 1 2 0 1 0 1 1 2\n",
      " 2 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 2 1 2 2 1 0 1 1 1 1 1 2 1 1 0 2 1 1\n",
      " 1 1 1 1 0 1 2 0 0 1 2 1 1 1 1 1 1 1 2 0 0 2 2 0 0 1 2 0 1 1 0 2 2 1 0 2 1\n",
      " 2 1 0 1 0 1 1 0 2 0 1 2 2 1 0 1 1 0 1 2 1 0 0 1 2 1 2 0 2 1 1 1 1 2 1 0 1\n",
      " 1 1 1 1 1 1 1 0 1 2 1 2 0 0 2 0 2 2 2 0 1 1 1 0 0 1 2 1 1 1 0 1 1 1 1 1 1\n",
      " 2 2 1 1 2 2 1 1 1 0 2 2 1 1 1 1 0 0 0 2 2 1 2 1 0 1 1 1 1 2 1 1 2 2 0 0 1\n",
      " 0 0 1 1 2 2 1 2 1 1 1 0 1 1 1 1 2 2 1 0 1 1 0 1 1 1 2 1 0 1 2 1 1 2 2 1 1\n",
      " 1 1 2 2 1 1 0 1 1 2 2 1 1 2 1 1 1 0 1 1 2 1 2 0 1 0 1 2 2 1 2 2 1 1 1 1 1\n",
      " 2 1 1 2 2 1 0 1 1 1 1 0 1 0 2 1 2 2 2 0 2 1 0 0 1 1 2 1 0 0 1 1 0 1 1 0 2\n",
      " 0 1 2 1 0 2 0 1 2 0 0 1 0 1 1 1 1 2 0 1 0 2 2 1 1 1 2 0 1 1 0 0 1 2 1 1 2\n",
      " 1 1 2 2 1 1 2 1 1 0 1 1 1 1 2 0 1 0 0 2 2 0 2 1 2 1 1 1 1 1 1 0 1 1 1 2 2\n",
      " 2 0 0 1 0 1 2 2 0 0 2 1 1 2 0 1 0 1 0 1 2 0 1 1 1 1 1 1 1 2 0 2 2 1 2 1 0\n",
      " 1 1 1 1 1 1 0 1 2 1 1 0 1 1 2 1 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Use sklearn utility to convert label strings to numbered index\n",
    "#============================================================================\n",
    "\n",
    "encoder = LabelEncoder()  \n",
    "encoder.fit(train_cat)\n",
    "\n",
    "#convert label strings to numbers\n",
    "y_train = encoder.transform(train_cat)\n",
    "y_test = encoder.transform(test_cat)\n",
    "\n",
    "#============================================================================\n",
    "#for each row in the data, each entry represents the value of the label\n",
    "#============================================================================\n",
    "print(y_train)\n",
    "\n",
    "#============================================================================\n",
    "# for example:  [2 1 1 2 1 1 0 ...  which corresponds to starter, other,\n",
    "# other, starter, other, other, brakes ...\n",
    "#============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf16bf-c977-4900-8aa0-e45a719141e2",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "We need to create labels (categories such as Brakes or Starter) for our test data, convert the labels to numbered index and then use one-hot encoding.\n",
    "\n",
    "**One hot encoding** allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. **The categories must be converted into numbers**. This is required for both input and output variables that are categorical.\n",
    "\n",
    "After we have converted the labels using one-hot encoding, we will be ready to build our main NLP model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26517820-122d-41e3-97f1-ef3b4a09289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (799, 1000)\n",
      "x_test shape: (200, 1000)\n",
      "y_train shape: (799, 3)\n",
      "y_test shape: (200, 3)\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Convert the labels to a one-hot representation\n",
    "#============================================================================\n",
    "\n",
    "num_classes = len(set(y_train))  # set() creates a unique set of objects\n",
    "y_train = to_categorical(y_train, num_classes)  \n",
    "\n",
    "#============================================================================\n",
    "# one hot encoding replaces the column of labels whose (values are 0 or 1 or 2)\n",
    "# with 3 columns each representing 1 label value.  For example, the label \n",
    "# 'other' is replaced by the vector 0 1 0, the label 'starter' is replaced by\n",
    "# the vector 0 0 1, the label 'brakes' is replaced by the vector 1 0 0\n",
    "#============================================================================\n",
    "\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "#============================================================================\n",
    "# Inspect the dimenstions of our training and test data\n",
    "#============================================================================\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "#one hot encoding\n",
    "#y_train shape: (159, 3)  159 rows, 3 columns\n",
    "#y_test shape: (40, 3)  40 rows, 3 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6416d-6cd6-49d8-9aca-2379b0702a43",
   "metadata": {},
   "source": [
    "\n",
    "## Building the model\n",
    "\n",
    "Once the model is trained, we can test our model by entering a repair issue (e.g. the brakes feel soft when I press on them) and check if the model has correctly characterized the repair issue. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc05d7a6-7e54-4208-b743-0ffbb866dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 0.9808 - accuracy: 0.4801 - val_loss: 0.5619 - val_accuracy: 0.9875\n",
      "Epoch 2/2\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.9599 - val_loss: 0.2227 - val_accuracy: 0.9875\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2539 - accuracy: 0.9550\n",
      "Test loss: 0.25385844707489014\n",
      "Test accuracy: 0.9549999833106995\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Build the model\n",
    "#============================================================================\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, input_shape=(max_words,), activation='relu'))  # Hidden layer with 512 nodes\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#============================================================================\n",
    "# relu, softmax, categorical_crossentropy are telling the model how to do some \n",
    "# internal calculations.  Softmax is telling the model to calculate \n",
    "# probabilities for each category in each document.  If you only had yes, \n",
    "# or no you would use sigmoid instead of softmax.\n",
    "#============================================================================\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#============================================================================\n",
    "# The variable, history, is normally used to plot learning curves.  fit \n",
    "# calculates the weights in the model.  \n",
    "# batch_size tells the internal calculations how many rows to process at one time\n",
    "# epochs is the number of times the model calculations will pass through \n",
    "# the entire data\n",
    "#============================================================================\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "history = model.fit(x_train, y_train,      \n",
    "                    batch_size=batch_size,  \n",
    "                    epochs=epochs,         \n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "#============================================================================\n",
    "#evaluate compares the model predictions with the actual known test values\n",
    "#============================================================================\n",
    "score = model.evaluate(x_test, y_test,       \n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "#============================================================================\n",
    "#print the test loss and accuracy of our model\n",
    "#============================================================================\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cecb48-13d4-4387-ac8f-37bfc797c8f4",
   "metadata": {},
   "source": [
    "\n",
    "## Let's test our model!\n",
    "\n",
    "Now that we have a model, we would like to generate a prediction (e.g. categorize the repair issue as:  Brakes, Starter or Other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157b9c35-7cf9-4302-b489-64fb2df23e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1000), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 1, 1000).\n",
      "returned prediction: starter\n"
     ]
    }
   ],
   "source": [
    "# Here's how to generate a prediction on individual examples\n",
    "text_labels = encoder.classes_   #ndarray of output values (labels or classes)  e.g. other, brakes, starter\n",
    "\n",
    "# Examine first 10 test samples of 445\n",
    "for i in range(len(test_cat)):\n",
    "    temp = x_test[i]\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]  #predicted class\n",
    "    #print(test_text.iloc[i][:50], \"...\")                # 50 char sample of text\n",
    "    #print('Actual label:' + test_cat.iloc[i])\n",
    "    #print(\"Predicted label: \" + predicted_label + \"\\n\")\n",
    "\n",
    "#test our model by inputing a new user claim =============   \n",
    "#print('================================  ENTER TEST PHRASE ====================================')\n",
    "#single_test_text = 'press brake pedal and car wont stop'  #text has to be a document, the brackets make it a document\n",
    "single_test_text = 'turn key over and hear a clicking sound' \n",
    "#single_test_text = 'there is fluid leaking from the engine' \n",
    "\n",
    "#print(\"Entered text: \" + str(single_test_text) + \"\\n\")\n",
    "\n",
    "#text_as_nparray = np.array([single_test_text])\n",
    "text_as_series = pd.Series(single_test_text)  #data conversions\n",
    "\n",
    "single_x_test = tokenize.texts_to_matrix(text_as_series)\n",
    "single_prediction = model.predict(np.array([single_x_test]))\n",
    "single_predicted_label = text_labels[np.argmax(single_prediction)]  #maps index of the prediction to the test labels array e.g. brakes\n",
    "#print(\"Predicted label: \" + single_predicted_label + \"\\n\")\n",
    "\n",
    "#======================================\n",
    "#test predication function\n",
    "#======================================\n",
    "#print('=====================test prediction function==================================')\n",
    "\n",
    "def predict(single_test_text):\n",
    "    text_as_series = pd.Series(single_test_text) #do a data conversion\n",
    "    single_x_test = tokenize.texts_to_matrix(text_as_series)\n",
    "    single_prediction = model.predict(np.array([single_x_test]))\n",
    "    single_predicted_label = text_labels[np.argmax(single_prediction)]  #maps index of the prediction to the test labels array e.g. brakes\n",
    "    return (single_predicted_label)\n",
    "\n",
    "prediction = predict(single_test_text)  #call the prediction function\n",
    "\n",
    "print('returned prediction: ' + prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e603b3-f4e2-428f-857b-8829d4ba270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other\n"
     ]
    }
   ],
   "source": [
    "prediction = predict('there is a pool of liquid under the car')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541f9a4-77ec-4878-87dc-4d0c77fdb56a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
