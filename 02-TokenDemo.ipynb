{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4eefc1c-4385-4ac6-9292-142eaac2f0f4",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of **converting text documentation into contextual vectors which contain numeric representations** (index of where those words occur in a work dictionary) of the words in the documents. \n",
    "\n",
    "In the following example, we define 5 'text' documents.  Each word in the document is gong to be placed into a dictionary and will be given a unique index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69d335e-e18a-41af-aeda-5448a6038281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts:  \n",
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "Document count:  \n",
      "5\n",
      "Word index:  \n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "Word docs:  \n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n",
      "Encoded docs matrix:  \n",
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import hashing_trick, text_to_word_sequence\n",
    "\n",
    "# define 5 'text'documents.  Here each string is considered a document.  Each word in the document is going to be put into a dictionary and is given a\n",
    "#unique index.\n",
    "\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!']\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)  # Basically, creates a word index and other info\n",
    "\n",
    "# summarize what was learned\n",
    "print(\"Word counts:  \\n{}\".format(t.word_counts))  # Dict of words and their count\n",
    "                                                    # In this example there are 8 unique words\n",
    "print(\"Document count:  \\n{}\".format(t.document_count)) # Number of docs in the fit\n",
    "print(\"Dictionary or Word index:  \\n{}\".format(t.word_index)) # Dict of words and their indexes. Index starts at 1\n",
    "                                                # since 0 is reserved.\n",
    "print(\"Word docs:  \\n{}\".format(t.word_docs)) # Dict of words and how many docs each appeared in\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs) # Matrix of words of each doc. Uses t.word_index for each doc.\n",
    "\n",
    "print(\"Encoded docs matrix:  \\n{}\".format(encoded_docs))  #this is the 'contextual vector form' of the original textual document 'docs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54540203-bea5-445e-a848-599bc5906a64",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Let's take a closer look at what happened in the above code!\n",
    "\n",
    "For example, the first row in the encoded matrix is:\n",
    "[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
    "\n",
    "Recall that our Word index is the following: \n",
    "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
    "\n",
    "The first row in the encoded matrix, contains two words, because there are 2 non-zero entries in the vector.  One of the words has an index of 2, and the other word has and index of 3 in the Word index.  Therefore the first document contains the 2 words 'well' and 'done'.\n",
    "\n",
    "**Note:  our matrix contains as many columns as there are unique words in the dictionary.**  The non-zero values in each row represent the word indexes of the all words in the document.\n",
    "\n",
    "Now that we understand Tokenization let's go back to our claims data and vectorize it.  Go back to the notebook **01-Create-Claims-Classification.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
