{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "In order to train a model to classify faults with metrobuses, we must first get our hands on some data. In this notebook we take a small data set, containing only 100 samples, and use [Markovify](https://github.com/jsvine/markovify) to simulate a larger data set, based on the original samples.\n",
    "\n",
    "We obtained a small data set by asking our colleagues to describe issues they currently have or had in the past, with their vehicles. \n",
    "\n",
    "**Note: you do not need to run this notebook in order to execute the remainder of the workshop. This notebook is solely for data generation. We have already generated the data for you, and it can be found in the `/dataset` folder.**\n",
    "\n",
    "Let's take a look at that data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.read_csv('dataset/response.csv') \n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data contains a range of information including, the time the error was recorded, and the type of issue, as well as a description of the problem. The information we are really interested in is the type of issue, and the description - let's go ahead and process this data, extracting only the info we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('')\n",
    "df['response']=df.iloc[:,3]+df.iloc[:,5]+df.iloc[:,6]\n",
    "df['issue'] = df.iloc[:,1]\n",
    "df['symptom'] = df.iloc[:,2] + df.iloc[:,4]\n",
    "subset = df.iloc[:,-3:]\n",
    "subset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much nicer! Let's explore our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.issue.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 3 categories of issue - 'Brakes', 'Starter' and 'Other'. \n",
    "\n",
    "How much data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['issue'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 109 rows - That's not likely to be enough to train a successful categorization model.\n",
    "\n",
    "Most of the data falls into the 'other' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rest of this notebook, we use Markovify to generate more data for each class of issue. Markovify is a Markov Chain generator, and we are going to use it to simulate more responses for each type of issue.\n",
    "\n",
    "First we install markovify, which is in our `nb-requirements.txt` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r nb-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_markov_type(data, issue):\n",
    "    return markovify.Text(data[data[\"issue\"] == issue].response, retain_original=False, state_size=2)\n",
    "\n",
    "#Function takes one of the 'issue' models and creates a randomly-generated sentence of length up to 100 characters.  \n",
    "def make_sentence(model, length=100):\n",
    "    return model.make_short_sentence(length, max_overlap_ratio = .7, max_overlap_total=15)\n",
    "\n",
    "#built models\n",
    "other_model = train_markov_type(subset, \"Other\")\n",
    "brakes_model = train_markov_type(subset, \"Brakes\")\n",
    "starter_model = train_markov_type(subset, \"Starter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these models with relative weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def generate_cases(models, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1] * len(models)\n",
    "    \n",
    "    choices = [] # Array of tuples of weight and models\n",
    "    \n",
    "    total_weight = float(sum(weights))\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        choices.append((float(sum(weights[0:i+1])) / total_weight, models[i]))\n",
    "    \n",
    "    # Return a tuple of model and category that are randomly selected by given weights.\n",
    "    def choose_model():\n",
    "        r = numpy.random.uniform()\n",
    "        for (model_weight, model) in choices:\n",
    "            if r <= model_weight:\n",
    "                return model\n",
    "        return choices[-1][1]\n",
    "\n",
    "\n",
    "    while True:\n",
    "        local_model = choose_model() \n",
    "        # local_model[0]) is the markovify model, local_model[1] is the category\n",
    "        yield make_sentence(local_model[0]), local_model[1]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this code to generate new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "generated_cases = generate_cases([(other_model,'other'), (brakes_model,'brakes'), (starter_model,'starter')], [14,7,7])\n",
    "\n",
    "# Tuples with sentence and category\n",
    "sentence_tuples = [next(generated_cases)  for i in range(1000)]  # create 200 sentence/category tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save these new issues and responses to file, and we will use this file later to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv file\n",
    "with open('dataset/testdata1.csv', 'w') as file:\n",
    "    writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "    writer.writerows(sentence_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have created a new data set, and we can transform this data and train a classification model. \n",
    "\n",
    "Let's head to notebook [01-Create-Claims-Classification.ipynb](01-Create-Claims-Classification.ipynb), where we process the data and train a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
