{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was generated in 2 steps:\n",
    "\n",
    "1.Initial Data was captured via Google Form which asked users for car issues they currently have or had in the past\n",
    "Classified that data into: brakes, starter, other\n",
    "\n",
    "2.Took this 'training set' and used Markovify to generate more data for our tutorial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in responses, only pull out client response, categorized issue and car symptom\n",
    "\n",
    "df = pd.read_csv('response.csv') \n",
    "df = df.fillna('')\n",
    "df['response']=df.iloc[:,3]+df.iloc[:,5]+df.iloc[:,6]\n",
    "df['issue'] = df.iloc[:,1]\n",
    "df['symptom'] = df.iloc[:,2] + df.iloc[:,4]\n",
    "subset = df.iloc[:,-3:]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#markovify is a simple, extensible Markov chain generator\n",
    "#Its primary use is for building Markov models of large corpora of text and generating random sentences from that.  \n",
    "\n",
    "#Function builds the model according to what issue (e.g. brakes, starter, other) is given\n",
    "def train_markov_type(data, issue):\n",
    "    return markovify.Text(data[data[\"issue\"] == issue].response, retain_original=False, state_size=2)\n",
    "\n",
    "#Function takes one of the 'issue' models and creates a randomly-generated sentence of length up to 200 characters.  Note only creates '1' sentence\n",
    "def make_sentence(model, length=100):\n",
    "    return model.make_short_sentence(length, max_overlap_ratio = .7, max_overlap_total=15)\n",
    "\n",
    "#built models\n",
    "other_model = train_markov_type(subset, \"Other\")\n",
    "brakes_model = train_markov_type(subset, \"Brakes\")\n",
    "starter_model = train_markov_type(subset, \"Starter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these models with relative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def generate_cases(models, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1] * len(models)\n",
    "    \n",
    "    choices = [] # Array of tuples of weight and models\n",
    "    \n",
    "    total_weight = float(sum(weights))\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        choices.append((float(sum(weights[0:i+1])) / total_weight, models[i]))\n",
    "    \n",
    "    # Return a tuple of model and category that are randomly selected by given weights.\n",
    "    def choose_model():\n",
    "        r = numpy.random.uniform()\n",
    "        for (model_weight, model) in choices:\n",
    "            if r <= model_weight:\n",
    "                return model\n",
    "        return choices[-1][1]\n",
    "\n",
    "\n",
    "    while True:\n",
    "        local_model = choose_model() \n",
    "        # local_model[0]) is the markovify model, local_model[1] is the category\n",
    "        yield make_sentence(local_model[0]), local_model[1]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate new sentences & classify them as:  other, brakes, starter.\n",
    "\n",
    "Store new sentences in file:  testdata1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "generated_cases = generate_cases([(other_model,'other'), (brakes_model,'brakes'), (starter_model,'starter')], [14,7,7])\n",
    "\n",
    "\n",
    "# Tuples with sentence and category\n",
    "sentence_tuples = [next(generated_cases)  for i in range(200)]  # create 200 sentence/category tuples\n",
    "\n",
    "# Write to csv file\n",
    "with open('testdata1.csv', 'w') as file:\n",
    "    writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "    writer.writerows(sentence_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have created a new data set.  There is however a problem we must overcome.  Machine Learning models cannot understand 'text'.  Therefore we must convert the textual data into some numeric form.\n",
    "\n",
    "We can do this, using Tokenization.  Jump to 02-TokenDemo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
